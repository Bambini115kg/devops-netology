### Домашнее задание к занятию "7.3. Основы и принцип работы Терраформ"

В виде результата работы пришлите:

**1. Вывод команды terraform workspace list.**

```
vagrant@vagrant:~/DZ73$ terraform workspace list
  default
* prod
  stage
```

**2. Вывод команды terraform plan для воркспейса prod.**
```
vagrant@vagrant:~/DZ73$ terraform plan

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # yandex_compute_instance.node-foreach["node01"] will be created
  + resource "yandex_compute_instance" "node-foreach" {
      + allow_stopping_for_update = true
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + hostname                  = (known after apply)
      + id                        = (known after apply)
      + metadata                  = {
          + "ssh-keys" = <<-EOT
                centos:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC7KRnWYZnRMUNzsyoIE8NKXtBu3m8Q/legWIv9yeXIgbwix0IfZXflOBXjcOGRm+IQyvQrfAjgT3G2quVqDru3LsxWHT6j/fzK9GYDr5bLsaTuCzYOK9+qIPRIulYCsPDZg7UzjBzI+mbgfmwGN5RyfxG9IutMhccLLIaGoLcBpjWtssPObO70kvedrtLjQo75KcYoNn4B7ya+MFx+i5NhJk58js5ZjVknVY56bBKY8S4Z1Hfy3waQwjFWhWQliFs2ZODrFpMYC6EW4ZKh2IcfsL0/xzqd9pu4nZ6xs6Gtl7TjEQkK8Vd1fnX2Wn9eL4YIogLeVEamf841erZxxuX3ccb59jmI6oPIDimeO8bkSwESddvgr6TePrlAWOTmSWcQAFHW5cGBaDxhEZZeK7KNyHUGrZKX++Jm8eCgx/8aiZ3yhRxWZK0j1QBmfwP1MlmYekMmCWito1lwEjduRi7EBS2mx9mJN/lIaN+H+TGsG8g5vTYk24ZPvw4OYi4uFxM= vagrant@localhost
            EOT
        }
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v1"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd8hqa9gq1d59afqonsf"
              + name        = (known after apply)
              + size        = 10
              + snapshot_id = (known after apply)
              + type        = "network-nvme"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = (known after apply)
          + ipv4               = true
          + ipv6               = (known after apply)
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = true
          + nat_ip_address     = (known after apply)
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + placement_policy {
          + host_affinity_rules = (known after apply)
          + placement_group_id  = (known after apply)
        }

      + resources {
          + core_fraction = 100
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy {
          + preemptible = (known after apply)
        }
    }

  # yandex_compute_instance.node-foreach["node02"] will be created
  + resource "yandex_compute_instance" "node-foreach" {
      + allow_stopping_for_update = true
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + hostname                  = (known after apply)
      + id                        = (known after apply)
      + metadata                  = {
          + "ssh-keys" = <<-EOT
                centos:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC7KRnWYZnRMUNzsyoIE8NKXtBu3m8Q/legWIv9yeXIgbwix0IfZXflOBXjcOGRm+IQyvQrfAjgT3G2quVqDru3LsxWHT6j/fzK9GYDr5bLsaTuCzYOK9+qIPRIulYCsPDZg7UzjBzI+mbgfmwGN5RyfxG9IutMhccLLIaGoLcBpjWtssPObO70kvedrtLjQo75KcYoNn4B7ya+MFx+i5NhJk58js5ZjVknVY56bBKY8S4Z1Hfy3waQwjFWhWQliFs2ZODrFpMYC6EW4ZKh2IcfsL0/xzqd9pu4nZ6xs6Gtl7TjEQkK8Vd1fnX2Wn9eL4YIogLeVEamf841erZxxuX3ccb59jmI6oPIDimeO8bkSwESddvgr6TePrlAWOTmSWcQAFHW5cGBaDxhEZZeK7KNyHUGrZKX++Jm8eCgx/8aiZ3yhRxWZK0j1QBmfwP1MlmYekMmCWito1lwEjduRi7EBS2mx9mJN/lIaN+H+TGsG8g5vTYk24ZPvw4OYi4uFxM= vagrant@localhost
            EOT
        }
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v1"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd8hqa9gq1d59afqonsf"
              + name        = (known after apply)
              + size        = 10
              + snapshot_id = (known after apply)
              + type        = "network-nvme"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = (known after apply)
          + ipv4               = true
          + ipv6               = (known after apply)
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = true
          + nat_ip_address     = (known after apply)
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + placement_policy {
          + host_affinity_rules = (known after apply)
          + placement_group_id  = (known after apply)
        }

      + resources {
          + core_fraction = 100
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy {
          + preemptible = (known after apply)
        }
    }

  # yandex_vpc_network.default will be created
  + resource "yandex_vpc_network" "default" {
      + created_at                = (known after apply)
      + default_security_group_id = (known after apply)
      + folder_id                 = (known after apply)
      + id                        = (known after apply)
      + labels                    = (known after apply)
      + name                      = (known after apply)
      + subnet_ids                = (known after apply)
    }

  # yandex_vpc_subnet.default will be created
  + resource "yandex_vpc_subnet" "default" {
      + created_at     = (known after apply)
      + folder_id      = (known after apply)
      + id             = (known after apply)
      + labels         = (known after apply)
      + name           = "subnet-prod"
      + network_id     = (known after apply)
      + v4_cidr_blocks = [
          + "192.168.101.0/24",
        ]
      + v6_cidr_blocks = (known after apply)
      + zone           = "ru-central1-a"
    }

Plan: 4 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + external_ip_address_nodes = [
      + (known after apply),
      + (known after apply),
    ]
  + internal_ip_address_nodes = [
      + (known after apply),
      + (known after apply),
    ]

───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run "terraform apply" now.

```

